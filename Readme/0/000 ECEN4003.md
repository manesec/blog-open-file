# ECEN4003 个人整合的知识点

Last Update : 2021年4月29日 - by Mane.

## 0x0h 编程部分

### **Python3 基本编程**

`Python 3` : [[菜鸟教程](https://www.runoob.com/python3/python3-tutorial.html)]

`Numpy` : [[User Guide](https://numpy.org/doc/stable/user/index.html)]  [[API](https://numpy.org/doc/stable/reference/index.html)]

`Pandas` : [[官网](https://pandas.pydata.org/)]

`Matplotlib` : [[Example](https://matplotlib.org/stable/gallery/index.html)] 

**熟用**：`Numpy` 和 `Pandas` 还有 `List` 之间的**互相转换**就可以了。

**知道**：如何用`Matplotlib` 画图，画图的时候再看文档就好了。

### Scikit-learn 基本编程

`Scikit-learn` : [[官网](https://scikit-learn.org/stable/)]

**要求**：要有自己的想法，会使用`数据`和参考`官方文档`生产出一些程序。

## 0x1h 建议学习时间

每天半小时，理论这东西挤在一起太多会腻。

注意：**不要背公式！！不要背公式！！不要背公式！！** 知道怎么用，是什么就行了，后面好多公式并不要求你推出来，程序人家也给你写好了，你只要会调用就行了！！

## 0x2h 无监督学习：分类，降维等

个人观点：下面是知识点，**标题是重点**，连接全都是给个参考的，看不懂自行去 Google 理解吧。

**Lecture 2**

1. 理论：什么是`有监督学习`与`无监督学习` (`Supervised` vs `Un-supervised` learning) ？
   - [监督学习和无监督学习 Supervised & Unsupervised（贪心学院 Greedy AI）- Youtube](https://www.youtube.com/watch?v=brslbBZ6N_M&ab_channel=%E8%B4%AA%E5%BF%83%E5%AD%A6%E9%99%A2GreedyAI)  推荐1.25倍速看，看完你会大概知道他们的大致的区别。
2. 有提到一下`Q-Learning`：
   - [什么是 Q-Learning - Youtube](https://www.youtube.com/watch?v=HTZ5xn12AL4) 用一个非常经典的例子告诉你什么是 `Q learning`
   - [极简 Qlearning 教程（附 Python 源码）- 知乎](https://zhuanlan.zhihu.com/p/29213893) 无视代码看图思考，你会发现他的特点是用数组放着权值来进行操作，缺点是如果用来下围棋，这个数组会非常的大，这个时候就可以把数组喂入 `NN`，`DQN`算法就是这样衍生而来。

**Lecture 3 - 1**

1. 先前知识：`数组`，去参考PPT上的数组运算就好
2. 理论：聚合算法一， `K-means`
   - [StatQuest: K-means clustering  - Youtube](https://www.youtube.com/watch?v=4b5d3muPQmA&ab_channel=StatQuestwithJoshStarmer)  这个人讲得非常好，推荐去看，一步到位。
   - **重点**：[Comparing different clustering algorithms on toy datasets - scikit-learn 官方文档](https://scikit-learn.org/stable/auto_examples/cluster/plot_cluster_comparison.html#sphx-glr-auto-examples-cluster-plot-cluster-comparison-py)  注意看图，图是重点，观察各个算法的比较。

**Lecture 3 - 2**

1. 理论：聚合算法二，`谱聚类算法 (Spectral clustering)`
   - [Lecture 34 — Spectral Clustering Three Steps (Advanced) | Stanford University - Youtube](https://www.youtube.com/watch?v=uxsDKhZHDcc) 讲的非常好的聚类
2. 理论：聚类算法三，`层次聚类 (Hierarchical clustering) `
   - [Clustering: K-means and Hierarchical - Youtube](https://youtu.be/QXOkPvFM6NU?t=789) `K-means` 看上面那个会比较清楚，直接跳到`Hierarchical` 部分就可以了。

**Lecture 4**

1. 理论：降维算法，使用`PCA`进行降维，虽然很鸡助，但很有用，看pro给的PPT就好了。
   - [StatQuest: PCA main ideas in only 5 minutes!!! - Youtube](https://www.youtube.com/watch?v=HMOI_lkzW08)  五分钟介绍 `PCA`
   - [StatQuest: Principal Component Analysis (PCA), Step-by-Step - Youtube](https://www.youtube.com/watch?v=FgakZw6K1QQ)  不懂再看加长版`PCA`介绍 

## 0x3h 有监督学习：优化，分类机等

个人观点：有监督学习通常比无监督学习要难，因为他要开始考虑标签的问题。

**Lecture 5**

1. 理论：`凸函数`和`凸优化`
   - 数学公式推理：[【机器学习系列】一文详解凸函数和凸优化，干货满满  - 知乎](https://zhuanlan.zhihu.com/p/51127402) 这个部分全都是数学推理，看不懂的自行谷歌解决吧，这里只是给个参考。
2. 理论：`梯度下降`
   - [神经网络：梯度下降 (Gradient Descent in Neural Nets) - Youtube](https://www.youtube.com/watch?v=9sJG7LjGCnI) 看完后你应该了解什么是梯度下降
3. 理论：`线性回归`，`多重线性回归`以及了解它的公式
   - [StatQuest: Linear Models Pt.1 - Linear Regression - Youtube](https://www.youtube.com/watch?v=nk2CQITm_eo&t=1420s) 手把手教你回归和用公司
   - [StatQuest: Linear Models Pt.1.5 - Multiple Regression - Youtube](https://www.youtube.com/watch?v=zITIFTsivN8) 多重线性回归
   - 当然除了`线性回归`还有一个`非线性回归`的，pro稍微提了一下而已。
4. 理论：什么是`过拟合`，`拟合`，`欠拟合`
   - [欠拟合、过拟合及如何防止过拟合 - 知乎](https://zhuanlan.zhihu.com/p/72038532) 了解什么是`过拟合`，`拟合`，`欠拟合`就可以了， **就看那张图片！！其他的不用看！！！（除非你有兴趣）**。
5. 理论：如何解决`过拟合`的问题
   1. [Lecture 7.1 — Regularization | The Problem Of Overfitting — [ Machine Learning | Andrew Ng - Youtube]](https://www.youtube.com/watch?v=u73PU6Qwl1I) 谷歌工程师，他讲的人工智能都很好。
   2. Regularization - `Cost Function`
      - [Lecture 7.2 — Regularization | Cost Function — [ Machine Learning | Andrew Ng | Stanford University - Youtube]](https://www.youtube.com/watch?v=KvtGD37Rm5I) 解决过拟合的问题
   3. Regularization - `Ridge`
      - [Regularization Part 1: Ridge (L2) Regression - Youtube](https://www.youtube.com/watch?v=Q81RR3yKn30)
   4. Regularization - `Lasso`
      - [Regularization Part 2: Lasso (L1) Regression - Youtube](https://www.youtube.com/watch?v=NGf0voTMlcs) 思考 λ 与模型的关系

**Lecture 6** 

个人观点：与上面的分类不同，这次是`有监督学习分类`，也就是`有标签学习的分类`，但难度会比上面多一级，为接下来喂入到`NN`打上基础。

1. 了解：`Regression` 和 `Classification` 的不同处

   - 一个`连续`，一个`离散`，参考 Lecture Note 第5页。

2. **重点**：分类的多种变体

   1. 参考 Lecture Note 第6页。
   2. 附加内容：每一种模型用的损失函数都不一样！！所以在喂入 `NN` 之前要看你的 `DATA` 是哪种变体。[更多可以参考这里](https://github.com/lyhue1991/eat_tensorflow2_in_30_days/blob/master/5-5%2C%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0losses.md#%E4%BA%8C%E5%86%85%E7%BD%AE%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0)

3. 理论：`KNN分类器`

   - [KNN 分类器 - CSDN](https://blog.csdn.net/autocyz/article/details/46786469) 看原理吧。

4. **理论和重点**：激活函数：`逻辑回归分类器 (Logistic regression classifier)`

   个人观点：这是一个新的概念，理解稍微辛苦一点，后期激活函数对`NN`和对模型很重要。

   - [什么是激励函数 (深度学习)? Why need activation functions (deep learning)? - Youtube](https://www.youtube.com/watch?v=tI9AbaBfnPc)
   - 有的叫激活函数，常见的函数可以[参考这里](https://github.com/lyhue1991/eat_tensorflow2_in_30_days/blob/master/5-3%2C%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0activation.md)。
   - 这里Pro 的 Lecture Note 只介绍了sigmoid function，Lecture 7 会介绍全部，事实上都要了解这些函数，以提供更好的`NN`模型。

5. 理论：`朴素贝叶斯分类器 (Naïve Bayes classifier)`

   - [Naive Bayes, Clearly Explained!!! - Youtube](https://www.youtube.com/watch?v=O2L2Uv9pdDA) 简单的说明什么是`朴素贝叶斯分类器`。

6. 理论：`决策树 (Decision tree)`

   - [StatQuest: Decision Trees - Youtube](https://www.youtube.com/watch?v=7VeUPuFGJHk) 简单的说明什么是`决策树`。
   - [Decision Tree Algorithm | Decision Tree in Python | Machine Learning Algorithms | Edureka - Youtube](https://youtu.be/qDcl-FRnwSU?t=637) 详细的介绍`决策树`。

7. **理论和重点**：`支持向量机 (Support vector machine)`

   个人观点：先了解`向量机`是用来干什么的，为什么要用到，然后再看原理。

   - [支持向量机（SVM）是什么意思？- 知乎](https://www.zhihu.com/question/21094489)

   - [Support Vector Machines Part 1 (of 3): Main Ideas!!! - Youtube](https://www.youtube.com/watch?v=efR1C6CvhmE)
   - [Support Vector Machines Part 2: The Polynomial Kernel (Part 2 of 3) - Youtube](https://www.youtube.com/watch?v=Toet3EiSFcM)
   - [Support Vector Machines Part 3: The Radial (RBF) Kernel (Part 3 of 3) - Youtube](https://www.youtube.com/watch?v=Qc5IyLW_hns)

8. 了解：集成学习，`Boosting` 和 `Bagging classifier`

   - [一文看懂集成学习（详解 bagging、boosting 以及他们的 4 点区别）](https://easyaitech.medium.com/%E4%B8%80%E6%96%87%E7%9C%8B%E6%87%82%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0-%E8%AF%A6%E8%A7%A3-bagging-boosting-%E4%BB%A5%E5%8F%8A%E4%BB%96%E4%BB%AC%E7%9A%84-4-%E7%82%B9%E5%8C%BA%E5%88%AB-6e3c72df05b8) 观察他们之间的区别就好了。
   - [What is the difference between bagging and boosting methods in ensemble learning? - Youtube](https://www.youtube.com/watch?v=UeYG64Hm7Es) 

9. 通过迭代弱分类器而产生最终的强分类器的算法：`AdaBoost` 分类器

   - [AdaBoost, Clearly Explained - Youtube](https://www.youtube.com/watch?v=LsK-xG1cLYA)

10. `随机树分类机：Random forest classifier`

    - [StatQuest: Random Forests Part 1 - Building, Using and Evaluating - Youtube](https://www.youtube.com/watch?v=J4Wdy0Wc_xQ)
    - [StatQuest: Random Forests Part 2: Missing data and clustering - Youtube](https://www.youtube.com/watch?v=nyxTdL_4Q-Q)

## 0x4h 有监督学习：神经网络 (NN)，深度学习（DL）

注意，下面所谓的人工智能训练出来的结果都是概率。

**Lecture 7**

1. 复习：回归模型

   - `线性回归`和`逻辑回归`，看回上面 Lecture 6 的第 4 点就好了

2. **重点和理论**：NN 模型里的`激活函数`

   - [形象的解释神经网络激活函数的作用是什么 - 简书](https://www.jianshu.com/p/1f8585a1dbb2)
   - **记住**：在NN模型里所使用的激活函数**都在隐藏层和输出层**。
   - 同样，看回上面 Lecture 6 的第4点就好了。
   - 详细请参考 0x5h 的第2个来源！。

3. 了解：各种NN的复杂模型

   - 模型越多，就越精准，但训练的时间会很长，需要喂入NN的数据就越多。
   - [一文看懂 25 个神经网络模型 - CSDN](https://blog.csdn.net/qq_35082030/article/details/73368962) **了解下就好，不需要全部看懂**。

4. 了解：什麼是人工智慧、機器學習和深度學習？

   1. 这里[有篇文章](https://chih-sheng-huang821.medium.com/%E4%BB%80%E9%BA%BC%E6%98%AF%E4%BA%BA%E5%B7%A5%E6%99%BA%E6%85%A7-%E6%A9%9F%E5%99%A8%E5%AD%B8%E7%BF%92%E5%92%8C%E6%B7%B1%E5%BA%A6%E5%AD%B8%E7%BF%92-587e6a0dc72a)，看那张图基本区分就可以了，这个是文科生的东西，对于理科生来说但实际上都是一个东西来的。

5. 理论：损失函数，分类交叉熵

   损失函数（我理解成误差函数，中文不知道为什么叫损失函数）用来评价模型的预测值和真实值不一样的程度的一个算法，损失函数越好，通常模型的性能越好，分类越准确，交叉熵越小。

   - [损失函数 - 交叉熵损失函数 - 知乎](https://zhuanlan.zhihu.com/p/35709485)  这篇是纯理论的东西，为的是告诉你这个函数做了什么。
   - 附加内容：[常见的损失函数 (loss function) 总结 - 知乎](https://zhuanlan.zhihu.com/p/58883095)

6. 理论：如何训练神经网络？

   - 梯度下降法，看回上面 Lecture 5 的第 2 点。

   - **重点**：`反向传播 (Back-propagation)` 的原理
   - [反向传播算法 - 机器之心](https://www.jiqizhixin.com/graph/technologies/7332347c-8073-4783-bfc1-1698a6257db3) 这个是看图简单的介绍下什么是`反向传播`。

   - [一文弄懂神经网络中的反向传播法 ——BackPropagation - cnblogs](https://www.cnblogs.com/charlotte77/p/5629865.html) 这里用到了数学公式和举了个简单的例子，当年我写韦sir的自动下棋AI就看了这篇文章，非常容易理解。

   - 训练的整个流程看 Lecture 7 第46页的 mind map 就好了。

7. **重点**：`卷积神经网络 CNN`

   在提取图片特征的时候经常用到，所以这个是个重点。

   - [Introducing convolutional neural networks (ML Zero to Hero - Part 3) - Youtube](https://www.youtube.com/watch?v=x_VrgWTKkiM) 谷歌工程师的视频，超棒。

8. 了解：什么是`循环神经网络 RNN`

   - [一文搞懂 RNN（循环神经网络）基础篇](https://zhuanlan.zhihu.com/p/30844905)  看下运行原理是什么就好了。

## 0x5h 强化学习，Q-Learning，DQN 等

偷偷告诉你，做单纯的数据处理的话，这部分可以跳过了。

**Lecture 8**

1. 了解：什么是`强化学习 RL` ？奖励函数是什么？

   - 想快点知道的，看Lecture 2 的第二个点 `Q-learning` 再来看下面的。

   - 了解：[强化学习 - Reinforcement learning | RL](https://easyai.tech/ai-definition/reinforcement-learning/) 了解什么是强化学习就好了。
   - 了解：[强化学习是什么？ - 知乎](https://www.zhihu.com/question/31140846)  看一些例子了解什么是强化学习。
   - 理论：[马可夫过程 - 知乎](https://zhuanlan.zhihu.com/p/50559214)  基本的Q - learning 运行原理
2. 理论：Q Learning ，DQN。

## 0x6h Application，数据处理

个人观点：思考的比较多，理论的东西越来越少了，开始抽象化了，并不是说一定要用 NN 去处理数据，因为有时候传统的算法会比喂入 NN 好，比如你喂入的数据不够，NN 没吃饱。

**Lecture 9**

1. 思考：有没有想过我们要用什么方法去预测未来的事情呢？
2. 思考：这些值会受到什么影响而改变？比如天气？心情？
3. 了解：基本的步骤：
   - 看 Lecture 9 第20页，这是数据处理比较常见的步骤。
4. 思考：传统方法和使用NN有什么差距？
5. 了解：梯度提升决策树 GBDT
   - [GBDT：梯度提升决策树 - 简书](https://www.jianshu.com/p/005a4e6ac775)  看看原理就好了

**Lecture 10**

1. 了解：何为异常？
   - 看 Lecture 10 第二页
2. 其他的看Lecture note吧，都是example。

## 0x7h 参考比较有用的文档

1. [不同聚类算法对比 - cnblogs](https://www.cnblogs.com/solong1989/p/9492467.html)  对比了`划分聚类（KMeans）`、`层次聚类`、`密度聚类（DBSCAN）`、`模型聚类`、`谱聚类`的优缺点。
2. [Activation Functions — All You Need To Know!](https://medium.com/analytics-vidhya/activation-functions-all-you-need-to-know-355a850d025e) [[微信公众号的中文翻译](https://mp.weixin.qq.com/s/KVSSntMwTJPl_7v8eYYhhQ)] 各个`激活函数`进行比较，以及它的优缺点。
3. [Machine Learning - Gitbook](https://shunliz.gitbooks.io/machine-learning/content/math/analytic/gradient_descent.html)  开源的ML书，大部分的内容都有。
4. [ovtenng 的文章 - segmentfault](https://segmentfault.com/u/ovtenng/articles)  一些图文并茂的文档。
