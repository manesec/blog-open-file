# ECEN4003 知识点

Last Update : 2021年4月11日 （正在更新）

## 0x0h 编程部分

### **Python3 基本编程**

[菜鸟教程](https://www.runoob.com/python3/python3-tutorial.html)  

Numpy : [[User Guide](https://numpy.org/doc/stable/user/index.html)]  [[API](https://numpy.org/doc/stable/reference/index.html)]

Pandas : [[官网](https://pandas.pydata.org/)]

Matplotlib : [[Example](https://matplotlib.org/stable/gallery/index.html)] 

**熟用**：Numpy 和 Pandas 还有 List 之间的**互相转换**就可以了。

**知道**：如何用Matplotlib 画图，画图的时候再看文档就好了。

### Scikit-learn 基本编程

Scikit-learn : [[官网](https://scikit-learn.org/stable/)]

## 0x1h 无监督学习：分类，降维等

个人观点：下面是知识点，看不懂的去Google找吧，下面只是给个参考。

**Lecture 2**

1. **理论**：什么是`有监督学习`与`无监督学习` (`Supervised` vs `Un-supervised` learning)
   - [监督学习和无监督学习 Supervised & Unsupervised（贪心学院 Greedy AI）- Youtube](https://www.youtube.com/watch?v=brslbBZ6N_M&ab_channel=%E8%B4%AA%E5%BF%83%E5%AD%A6%E9%99%A2GreedyAI)  推荐1.25倍速看，看完你会大概知道他们的大致的区别。
2. 有提到一下`Q-Learning`：
   - [什么是 Q-Learning - Youtube](https://www.youtube.com/watch?v=HTZ5xn12AL4) 用一个非常经典的例子告诉你什么是Q learning
   - [极简 Qlearning 教程（附 Python 源码）- 知乎](https://zhuanlan.zhihu.com/p/29213893) 无视代码看图思考，你会发现他的特点是用数组放着权值来进行操作，缺点是如果用来下围棋，这个数组会非常的大，这个时候就可以把数组喂入NN，DNN算法就是这样衍生而来。

**Lecture 3 - 1**

1. 先前知识：`数组`，去参考PPT上的数组运算就好
2. 理论：聚合算法一， `K-means`
   - [StatQuest: K-means clustering  - Youtube](https://www.youtube.com/watch?v=4b5d3muPQmA&ab_channel=StatQuestwithJoshStarmer)  这个人讲得非常好，推荐去看，一步到位。
   - **重点**：[Comparing different clustering algorithms on toy datasets - scikit-learn 官方文档](https://scikit-learn.org/stable/auto_examples/cluster/plot_cluster_comparison.html#sphx-glr-auto-examples-cluster-plot-cluster-comparison-py)  注意看图，图是重点，观察各个算法的比较。

**Lecture 3 - 2**

1. 理论：聚合算法二，`谱聚类算法 (Spectral clustering)`
   - [Lecture 34 — Spectral Clustering Three Steps (Advanced) | Stanford University - Youtube](https://www.youtube.com/watch?v=uxsDKhZHDcc) 讲的非常好的聚类
2. 理论：聚类算法三，`层次聚类 (Hierarchical clustering) `
   - [Clustering: K-means and Hierarchical - Youtube](https://youtu.be/QXOkPvFM6NU?t=789) K-means 看上面那个会比较清楚，直接跳到Hierarchical 部分就可以了。

**Lecture 4**

1. 理论：使用`PCA`进行降维，虽然很鸡助，但很有用，看pro给的PPT就好了。
   - [StatQuest: PCA main ideas in only 5 minutes!!! - Youtube](https://www.youtube.com/watch?v=HMOI_lkzW08)  五分钟介绍PCA
   - [StatQuest: Principal Component Analysis (PCA), Step-by-Step - Youtube](https://www.youtube.com/watch?v=FgakZw6K1QQ)  加长版介绍PCA

## 0x2h 有监督学习：优化，分类机等

个人观点：有监督学习通常比无监督学习要难，因为他要开始考虑标签的问题。

**Lecture 5**

1. 理论：`凸函数`和`凸优化`
   - 数学公式推理：[【机器学习系列】一文详解凸函数和凸优化，干货满满  - 知乎](https://zhuanlan.zhihu.com/p/51127402) 这个部分全都是数学推理，看不懂的自行谷歌解决吧，这里只是给个参考。
2. 理论：`梯度下降`
   - [神经网络：梯度下降 (Gradient Descent in Neural Nets) - Youtube](https://www.youtube.com/watch?v=9sJG7LjGCnI) 看完后你应该了解什么是梯度下降
3. 理论：`线性回归`，`多重线性回归`以及了解它的公式
   - [StatQuest: Linear Models Pt.1 - Linear Regression - Youtube](https://www.youtube.com/watch?v=nk2CQITm_eo&t=1420s) 手把手教你回归和用公司
   - [StatQuest: Linear Models Pt.1.5 - Multiple Regression - Youtube](https://www.youtube.com/watch?v=zITIFTsivN8) 多重线性回归
   - 当然除了线性回归还有一个非线性的，pro稍微提了一下而已。
4. 理论：什么是`过拟合`，`拟合`，`欠拟合`
   - [欠拟合、过拟合及如何防止过拟合 - 知乎](https://zhuanlan.zhihu.com/p/72038532) 了解什么是过拟合，拟合，欠拟合就可以了， **就看那张图片！！其他的不用看！！！（除非你有兴趣）**。
5. 理论：如何解决`过拟合`的问题
   1. [Lecture 7.1 — Regularization | The Problem Of Overfitting — [ Machine Learning | Andrew Ng - Youtube]](https://www.youtube.com/watch?v=u73PU6Qwl1I) 为什么会出现谷歌工程师，他讲的人工智能都很好。
   2. Regularization - `Cost Function`
      - [Lecture 7.2 — Regularization | Cost Function — [ Machine Learning | Andrew Ng | Stanford University - Youtube]](https://www.youtube.com/watch?v=KvtGD37Rm5I) 解决过拟合的问题
   3. Regularization - `Ridge`
      - [Regularization Part 1: Ridge (L2) Regression - Youtube](https://www.youtube.com/watch?v=Q81RR3yKn30)
   4. Regularization - `Lasso`
      - [Regularization Part 2: Lasso (L1) Regression - Youtube](https://www.youtube.com/watch?v=NGf0voTMlcs) 思考 λ 与模型的关系

**Lecture 6** 

个人观点：与上面的分类不同，这次是有监督学习分类，也就是有标签学习分类，但难度会比上面多一级，为接下来喂入到NN打上基础。

1. 了解：Regression 和 Classification 的不同处

   - 一个连续，一个离散，参考 Lecture Note 第5页。

2. **重点**：分类的多种变体

   1. 参考 Lecture Note 第6页。
   2. 附加内容：每一种模型用的损失函数都不一样！！所以在喂入 NN 之前要看你的DATA 是哪种变体。[更多可以参考这里](https://github.com/lyhue1991/eat_tensorflow2_in_30_days/blob/master/5-5%2C%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0losses.md#%E4%BA%8C%E5%86%85%E7%BD%AE%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0)

3. 理论：KNN分类器

   - [KNN 分类器 - CSDN](https://blog.csdn.net/autocyz/article/details/46786469) 看原理吧。

4. **理论和重点**：激活函数：`逻辑回归分类器 (Logistic regression classifier)`

   个人观点：这是一个新的概念，理解稍微辛苦一点，后期激活函数对NN和对模型很重要。

   - [什么是激励函数 (深度学习)? Why need activation functions (deep learning)? - Youtube](https://www.youtube.com/watch?v=tI9AbaBfnPc)
   - 有的叫激活函数，常见的函数可以[参考这里](https://github.com/lyhue1991/eat_tensorflow2_in_30_days/blob/master/5-3%2C%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0activation.md)。
   - 这里Pro 的 Lecture Note 只介绍了sigmoid function，Lecture 7 会介绍全部，事实上都要了解这些函数，以提供更好的NN模型。

5. 理论：`朴素贝叶斯分类器 (Naïve Bayes classifier)`

   - [Naive Bayes, Clearly Explained!!! - Youtube](https://www.youtube.com/watch?v=O2L2Uv9pdDA) 简单的说明什么是朴素贝叶斯分类器。

6. 理论：`决策树 (Decision tree)`

   - [StatQuest: Decision Trees - Youtube](https://www.youtube.com/watch?v=7VeUPuFGJHk) 简单的说明什么是决策树。

7. **理论和重点**：`向量机 (Support vector machine)`

   个人观点：先了解向量机是用来干什么的，为什么要用到，然后再看原理。

   - [支持向量机（SVM）是什么意思？- 知乎](https://www.zhihu.com/question/21094489)

   - [Support Vector Machines Part 1 (of 3): Main Ideas!!! - Youtube](https://www.youtube.com/watch?v=7VeUPuFGJHk)
   - [Support Vector Machines Part 2: The Polynomial Kernel (Part 2 of 3) - Youtube](https://www.youtube.com/watch?v=Toet3EiSFcM)
   - [Support Vector Machines Part 3: The Radial (RBF) Kernel (Part 3 of 3) - Youtube](https://www.youtube.com/watch?v=Qc5IyLW_hns)

8. 了解：集成学习

   1. Boosting 和 Bagging classifier
      - [一文看懂集成学习（详解 bagging、boosting 以及他们的 4 点区别）](https://easyaitech.medium.com/%E4%B8%80%E6%96%87%E7%9C%8B%E6%87%82%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0-%E8%AF%A6%E8%A7%A3-bagging-boosting-%E4%BB%A5%E5%8F%8A%E4%BB%96%E4%BB%AC%E7%9A%84-4-%E7%82%B9%E5%8C%BA%E5%88%AB-6e3c72df05b8) 观察他们之间的区别就好了。

   2. 通过迭代弱分类器而产生最终的强分类器的算法：AdaBoost 分类器
      - [AdaBoost, Clearly Explained - Youtube](https://www.youtube.com/watch?v=LsK-xG1cLYA)
   3. 随机树分类机：Random forest classifier
      - [StatQuest: Random Forests Part 1 - Building, Using and Evaluating - Youtube](https://www.youtube.com/watch?v=J4Wdy0Wc_xQ)
      - [StatQuest: Random Forests Part 2: Missing data and clustering - Youtube](https://www.youtube.com/watch?v=nyxTdL_4Q-Q)

## 0x3h 有监督学习：神经网络 (NN)，深度学习（DL）

**Lecture 7**

1. 复习：回归模型

   - 线性回归和逻辑回归，看回上面 Lecture 6 的第 4 点就好了

2. **重点和理论**：NN 模型里的激活函数

   - [形象的解释神经网络激活函数的作用是什么 - 简书](https://www.jianshu.com/p/1f8585a1dbb2)
   - 记住：在NN模型里所使用的激活函数**都在隐藏层和输出层**。
   - 同样，看回上面 Lecture 6 的第4点就好了。

3. 了解：各种NN的复杂模型

   - 模型越多，就越精准，但训练的时间会很长，需要喂入NN的数据就越多。
   - [一文看懂 25 个神经网络模型 - CSDN](https://blog.csdn.net/qq_35082030/article/details/73368962) **了解下就好，不需要全部看懂**。

4. 理论：训练神经网络

   - 梯度下降法，看回上面 Lecture 5 的第 2 点。

   - **重点**：`反向传播 (Back-propagation)`

     - [反向传播算法 - 机器之心](https://www.jiqizhixin.com/graph/technologies/7332347c-8073-4783-bfc1-1698a6257db3) 这个是看图简单的介绍下什么是反向传播。

     - [一文弄懂神经网络中的反向传播法 ——BackPropagation - cnblogs](https://www.cnblogs.com/charlotte77/p/5629865.html) 这里用到了数学公式和举了个简单的例子，当年我写韦sir的自动下棋AI就看了这篇文章，非常容易理解。

   - 训练的整个流程看 Lecture 7 第46页的mind map 就好了。

5. **重点**：卷积神经网络 CNN

   在提取图片特征的时候经常用到，所以这个是个重点。

   1. [Introducing convolutional neural networks (ML Zero to Hero - Part 3) - Youtube](https://www.youtube.com/watch?v=x_VrgWTKkiM) 谷歌工程师的视频，超棒。

Lecture ... 待更新
